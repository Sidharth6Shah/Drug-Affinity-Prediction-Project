{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 7: GNN Model for Protein-Ligand Binding Affinity Prediction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self, nodeFeatureDimension, proteinEmbeddingDimension, hiddenDimension = 128, gnnLayers = 3, outputDimension = 1):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = GCNConv(nodeFeatureDimension, hiddenDimension)\n",
    "        self.conv2 = GCNConv(hiddenDimension, hiddenDimension)\n",
    "        self.conv3 = GCNConv(hiddenDimension, hiddenDimension)\n",
    "\n",
    "        #Multilayer Perceptron for prediction (combining gnn output with protein embeddings)\n",
    "        combinedDimension = hiddenDimension + proteinEmbeddingDimension\n",
    "        self.fc1 = nn.Linear(combinedDimension, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, outputDimension)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, graphData, proteinEmbedding):\n",
    "        x = graphData.x\n",
    "        edgeIndex = graphData.edge_index\n",
    "        batch = graphData.batch\n",
    "\n",
    "        #gnn layers w/ ReLu activation\n",
    "        x = self.conv1(x, edgeIndex)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edgeIndex)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edgeIndex)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        #concatenating gnn embedding w/ protein embedding\n",
    "        combined = torch.cat([x, proteinEmbedding], dim=1)\n",
    "\n",
    "        #multilayer perceptron for actual prediction\n",
    "        x = self.fc1(combined)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class proteinLigandDataset(Dataset):\n",
    "    def __init__(self, df, ligandGraphs, proteinEmbeddings):\n",
    "        super(proteinLigandDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.ligandGraphs = ligandGraphs\n",
    "        self.proteinEmbeddings = proteinEmbeddings\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def get(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        #SMILES and protein sequences\n",
    "        SMILES = row['Ligand SMILES']\n",
    "        proteinSequence = row['BindingDB Target Chain Sequence 1']\n",
    "        label = row['pKd']\n",
    "\n",
    "        #pull cached embeddings\n",
    "        graphDictionary = self.ligandGraphs[SMILES]\n",
    "        proteinEmb = self.proteinEmbeddings[proteinSequence]\n",
    "\n",
    "        #convert graph dictionaries to pytorch geometric data objects\n",
    "        nodeFeatures = torch.tensor(graphDictionary['nodeFeatures'], dtype=torch.float)\n",
    "        edgeIndex = torch.tensor(graphDictionary['edgeIndex'], dtype=torch.long).t().contiguous()\n",
    "\n",
    "        #create data object\n",
    "        graphData = Data(x = nodeFeatures, edge_index = edgeIndex)\n",
    "\n",
    "        #convert protein embeddings to tensors\n",
    "        proteinTensor = torch.tensor(proteinEmb, dtype=torch.float).unsqueeze(0)\n",
    "        labelTensor = torch.tensor([label], dtype=torch.float)\n",
    "\n",
    "        return graphData, proteinTensor, labelTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 34219 train, 10214 val, 7351 test\n",
      "After filtering: 34207 train, 10211 val, 7345 test\n",
      "Loaded 17049 unique ligand graphs\n",
      "Loaded 1065 unique protein embeddings\n"
     ]
    }
   ],
   "source": [
    "#loading embeddings\n",
    "train_df = pd.read_csv('../data/splits/train.csv', low_memory=False)\n",
    "val_df = pd.read_csv('../data/splits/val.csv', low_memory=False)\n",
    "test_df = pd.read_csv('../data/splits/test.csv', low_memory=False)\n",
    "\n",
    "# Load cached ligand graphs\n",
    "with open('../embeddings/ligands/ligand_graphs.pkl', 'rb') as f:\n",
    "    ligandGraphs = pickle.load(f)\n",
    "\n",
    "# Load cached protein embeddings\n",
    "with open('../embeddings/proteins/protein_embeddings.pkl', 'rb') as f:\n",
    "    proteinEmbeddings = pickle.load(f)\n",
    "\n",
    "# Filter out rows with missing graphs or embeddings\n",
    "print(f'Before filtering: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test')\n",
    "\n",
    "train_df = train_df[train_df['Ligand SMILES'].isin(ligandGraphs.keys())]\n",
    "train_df = train_df[train_df['BindingDB Target Chain Sequence 1'].isin(proteinEmbeddings.keys())]\n",
    "\n",
    "val_df = val_df[val_df['Ligand SMILES'].isin(ligandGraphs.keys())]\n",
    "val_df = val_df[val_df['BindingDB Target Chain Sequence 1'].isin(proteinEmbeddings.keys())]\n",
    "\n",
    "test_df = test_df[test_df['Ligand SMILES'].isin(ligandGraphs.keys())]\n",
    "test_df = test_df[test_df['BindingDB Target Chain Sequence 1'].isin(proteinEmbeddings.keys())]\n",
    "\n",
    "print(f'After filtering: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test')\n",
    "print(f'Loaded {len(ligandGraphs)} unique ligand graphs')\n",
    "print(f'Loaded {len(proteinEmbeddings)} unique protein embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and dataloaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_dataset = proteinLigandDataset(train_df, ligandGraphs, proteinEmbeddings)\n",
    "val_dataset = proteinLigandDataset(val_df, ligandGraphs, proteinEmbeddings)\n",
    "test_dataset = proteinLigandDataset(test_df, ligandGraphs, proteinEmbeddings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature dimensions from sample\n",
    "sample_graph, sample_protein, sample_label = train_dataset[0]\n",
    "nodeFeatureDim = sample_graph.x.shape[1]\n",
    "proteinEmbeddingDim = sample_protein.shape[1]\n",
    "\n",
    "# initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn_model = model(nodeFeatureDim, proteinEmbeddingDim, hiddenDimension=128)\n",
    "gnn_model = gnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training function\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for batch_graph, batch_protein, batch_label in loader:\n        batch_graph = batch_graph.to(device)\n        batch_protein = batch_protein.to(device).squeeze(1)\n        batch_label = batch_label.to(device)\n        \n        optimizer.zero_grad()\n        predictions = model(batch_graph, batch_protein)\n        loss = criterion(predictions, batch_label)\n        \n        # Check for NaN\n        if torch.isnan(loss):\n            print(\"NaN loss detected, skipping batch\")\n            continue\n            \n        loss.backward()\n        \n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\n# Evaluation function\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch_graph, batch_protein, batch_label in loader:\n            batch_graph = batch_graph.to(device)\n            batch_protein = batch_protein.to(device).squeeze(1)\n            batch_label = batch_label.to(device)\n            \n            predictions = model(batch_graph, batch_protein)\n            loss = criterion(predictions, batch_label)\n            \n            total_loss += loss.item()\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(batch_label.cpu().numpy())\n    \n    # Convert to arrays and filter out NaN values\n    all_preds = np.array(all_preds).flatten()\n    all_labels = np.array(all_labels).flatten()\n    \n    # Remove NaN predictions\n    valid_mask = ~np.isnan(all_preds)\n    all_preds = all_preds[valid_mask]\n    all_labels = all_labels[valid_mask]\n    \n    if len(all_preds) == 0:\n        return float('inf'), float('inf'), 0.0\n    \n    avg_loss = total_loss / len(loader)\n    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n    r2 = r2_score(all_labels, all_preds)\n    \n    return avg_loss, rmse, r2\n\n# Import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch\n",
      "Training for 50 epochs\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[32m     32\u001b[39m     train_loss = train_epoch(gnn_model, train_loader, optimizer, criterion, device)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     val_loss, val_rmse, val_r2 = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     train_losses.append(train_loss)\n\u001b[32m     36\u001b[39m     val_losses.append(val_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, loader, criterion, device)\u001b[39m\n\u001b[32m     39\u001b[39m         all_labels.extend(batch_label.cpu().numpy())\n\u001b[32m     41\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m rmse = np.sqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     43\u001b[39m r2 = r2_score(all_labels, all_preds)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, rmse, r2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:565\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    561\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    563\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    564\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m )\n\u001b[32m    569\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    570\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:198\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures that y_true, y_pred, and sample_weight correspond to the same\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03mregression task.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m y_type, y_true, y_pred, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# _check_reg_targets does not accept sample_weight as input.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# Convert sample_weight's data type separately to match dtype_name.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:105\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    102\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m    104\u001b[39m check_consistent_length(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m y_true = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_true.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "start_epoch = 0\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_path = '../models/gnn_checkpoint.pt'\n",
    "if Path(checkpoint_path).exists():\n",
    "    print('Loading checkpoint...')\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    gnn_model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_rmses = checkpoint['val_rmses']\n",
    "    best_val_rmse = checkpoint['best_val_rmse']\n",
    "    print(f'Resuming from epoch {start_epoch}')\n",
    "else:\n",
    "    best_val_rmse = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_rmses = []\n",
    "    print(f'Starting training from scratch')\n",
    "\n",
    "print(f'Training for {num_epochs} epochs')\n",
    "print('-' * 60)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_loss = train_epoch(gnn_model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_rmse, val_r2 = evaluate(gnn_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        torch.save(gnn_model.state_dict(), '../models/best_gnn_model.pt')\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': gnn_model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_rmses': val_rmses,\n",
    "            'best_val_rmse': best_val_rmse\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.4f} | Val R²: {val_r2:.4f} | [Checkpoint saved]')\n",
    "    else:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.4f} | Val R²: {val_r2:.4f}')\n",
    "\n",
    "print('-' * 60)\n",
    "print(f'Training complete! Best validation RMSE: {best_val_rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}